{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4647c0d",
   "metadata": {},
   "source": [
    "# Семинар 12. Свертки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16034d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import cm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae122a",
   "metadata": {},
   "source": [
    "Загрузим квокку!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O quokka.jpg https://www.meme-arsenal.com/memes/5851dce2a0718490da96a3819221a01a.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"quokka.jpg\")\n",
    "print(f\"Image format: {img.format}; shape: {img.size}; color scheme: {img.mode}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178c672",
   "metadata": {},
   "source": [
    "Мы знаем, что цветное изображение состоит из 3 числовых матриц или трехмерного тензора. Каждая матрица соответствует одному из 3 базовых цветов: красному, зеленому и синему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85048c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем тензор\n",
    "img_matrix = np.array(img)\n",
    "\n",
    "# (высота, ширина, число каналов)\n",
    "print(f\"Image matrix shape: {img_matrix.shape}\")\n",
    "\n",
    "plt.imshow(img_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508e586",
   "metadata": {},
   "source": [
    "## Операция свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82561944",
   "metadata": {},
   "source": [
    "В PyTorch свёрточный слой представлен в модуле `nn` функцией [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "\n",
    "Нам пригодится знать про эти параметры: \n",
    "\n",
    "- количество входных каналов `in_channels`\n",
    "- количество выходных каналов `out_channels`\n",
    "- размер ядра `kernel_size`\n",
    "- шаг `stride`\n",
    "- паддинг `padding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a2a04",
   "metadata": {},
   "source": [
    "**Размер ядра** - `int`, если ядро квадратное и кортеж из двух чисел, если ядро прямоугольное. Задает размер фильтра, с которым производится свертка изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9866691",
   "metadata": {},
   "source": [
    "![no_padding_no_strides.gif](no_padding_no_strides.gif)\n",
    "\n",
    "Эта и следующие анимации взяты [здесь](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb6535",
   "metadata": {},
   "source": [
    "**Шаг** - задает шаг, в пикселях, на который сдвигается фильтр. `int`, если по горизонтали и вертикали сдвигается на одно и то же число. Кортеж из двух чисел, если сдвиги разные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f219a",
   "metadata": {},
   "source": [
    "![no_padding_strides.gif](no_padding_strides.gif)\n",
    "\n",
    "Шаг: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafe52b",
   "metadata": {},
   "source": [
    "**Паддинг** - количество пикселей, которыми дополняется изображение. Аналогично шагу и размеру ядра, может быть, как `int`, так и кортежем из двух чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c89e3",
   "metadata": {},
   "source": [
    "**Half pading**\n",
    "![same_padding_no_strides.gif](same_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c8e36",
   "metadata": {},
   "source": [
    "### Свертка изображения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8629f2a",
   "metadata": {},
   "source": [
    "Применим оператор Собеля для детектирования границ на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabdc6d",
   "metadata": {},
   "source": [
    "Откуда взялись эти цифры и почему с ними подсвечиваются границы, можно посмотреть [здесь](https://nrsyed.com/2018/02/18/edge-detection-in-images-how-to-derive-the-sobel-operator/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfd2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4bd6a",
   "metadata": {},
   "source": [
    "Конвертируем изображение в нужный формат для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.tensor([img_matrix], dtype=torch.float)\n",
    "img_tensor.size()  # (число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = img_tensor.permute(0, 3, 1, 2)\n",
    "img_tensor.size()  # (число изображений, число каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632765e3",
   "metadata": {},
   "source": [
    "Зададим оператор Собеля для детектирования горизонтальных границ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_hor = [[-1, -2, -1], \n",
    "             [ 0,  0,  0], \n",
    "             [ 1,  2,  1]]\n",
    "\n",
    "# одна матрица на каждый канал картинки\n",
    "kernels  = [[sobel_hor, sobel_hor, sobel_hor]]\n",
    "kernels = torch.tensor(kernels, dtype=torch.float)\n",
    "kernels.size() # (число выходных каналов, число входных каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# свернём картинку с подготовленным ядром свёртки\n",
    "img_conv_hor = conv2d(img_tensor, kernels)\n",
    "img_conv_hor = img_conv_hor.permute(0, 2, 3, 1)\n",
    "img_conv_hor.size()  # (число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(torch.abs(img_conv_hor[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56268f2",
   "metadata": {},
   "source": [
    "Зададим оператор Собеля для детектирования вертикальных границ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f300fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_ver = [[-1, 0, 1], \n",
    "             [-2, 0, 2], \n",
    "             [-1, 0, 1]]\n",
    "\n",
    "# одна матрица на каждый канал картинки\n",
    "kernel  = [[sobel_ver, sobel_ver, sobel_ver]]\n",
    "kernel = torch.tensor(kernel, dtype=torch.float)\n",
    "kernel.size() #(число выходных каналов, число входных каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b394bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv_ver = conv2d(img_tensor, kernel)\n",
    "\n",
    "img_conv_ver = img_conv_ver.permute(0, 2, 3, 1)\n",
    "img_conv_ver.size() #(число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cce1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(torch.abs(img_conv_ver[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a1a23",
   "metadata": {},
   "source": [
    "Объединим два изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv = torch.sqrt(img_conv_ver**2 + img_conv_hor**2)\n",
    "\n",
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(img_conv[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbf23f",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "По аналогии с примером выше, сверните изображение со случайным ядром такого же размера.\n",
    "\n",
    "**Подсказка:** используйте `torch.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa710eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\n",
    "# свертка изображения\n",
    "img_conv_ver = conv2d(img_tensor, kernels_random)\n",
    "img_conv_ver = img_conv_ver.permute(0, 2, 3, 1)\n",
    "\n",
    "# рисуем результат\n",
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(img_conv_ver[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2ca15",
   "metadata": {},
   "source": [
    "### Полносвязная нейронная сеть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c4e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f1b19",
   "metadata": {},
   "source": [
    "Будем работать с датасетом [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), в котором содержатся изображения 10 классов размером 32 на 32 пикселя с тремя каналами. В тренировочной выборке 50000 изображений, а в тестовой 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3606388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400d6768985e49c885049adfa4b0ecde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=True,                             \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    valset, \n",
    "    batch_size=16,\n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eae5d6",
   "metadata": {},
   "source": [
    "Обучаем полносвязную нейронную сеть для классификации изображений. Она будет работать долго и не классно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),             \n",
    "    nn.Linear(32 * 32 * 3, 512),  \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(512, 128), \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(128, 10),       \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # создаем оптимизатор и передаем туда параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5f3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, n_epochs=5):\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # тренировка\n",
    "        pbar = tqdm(train_dataloader)\n",
    "        for x_train, y_train in pbar:\n",
    "            y_pred = model(x_train)\n",
    "            loss = F.cross_entropy(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar.set_description(f'Loss {loss.data}')\n",
    "\n",
    "        # валидация\n",
    "        if epoch % 2 == 0:\n",
    "            val_loss = []\n",
    "            val_accuracy = []\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in tqdm(val_dataloader):\n",
    "                    y_pred = model(x_val)\n",
    "                    loss = F.cross_entropy(y_pred, y_val)\n",
    "                    val_loss.append(loss.numpy())\n",
    "                    val_accuracy.extend((torch.argmax(y_pred, dim=-1) == y_val))\n",
    "                \n",
    "            # печатаем метрики\n",
    "            print(f\"Epoch: {epoch}, loss: {np.mean(val_loss)}, accuracy: {np.mean(val_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faadbe5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f78f04aa1948d5b76d5735d5f47946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61afa4db2354ccabb498933ecaa5809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.578218698501587, accuracy: 0.4455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8286198de29a4f67b269570c5093f525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af6592683c94bdaa61508613b33035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5415806e344eb8b091f4ede1162fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss: 1.4013235569000244, accuracy: 0.5074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbecb964e6a249759a3d0756fec78d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d454110fa614609b8d204c44983a9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff73659aec3c43a4a479874e32cdc2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss: 1.3576728105545044, accuracy: 0.5221\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5306df",
   "metadata": {},
   "source": [
    "### Сверточный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348f940",
   "metadata": {},
   "source": [
    "Добавим в нашу сеть сверточный слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f59628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),  \n",
    "    nn.Flatten(),                 \n",
    "    nn.Linear(7 * 7 * 10, 128),       \n",
    "    nn.ReLU(),                    \n",
    "    nn.Linear(128, 10)        \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8456e154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70307e8017364d0d8017731b94c8475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cbbdc1ee7a4046b9723e79b7f7a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.4431393146514893, accuracy: 0.4861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c797b02cc6748fe9cab327ea56216bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5250768b61748e4ac34bc0fccc03f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe3f9fa319347a7855a0148b4a34e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss: 1.221283197402954, accuracy: 0.5665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3165249d5d54591a3ac8f433534de4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7069edbccba3486bae64d35bcafb8de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38aad14d25648c1a3b776253067d826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss: 1.0898840427398682, accuracy: 0.6201\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b71e3f",
   "metadata": {},
   "source": [
    "### Задание 2 (домашка)\n",
    "\n",
    "По аналогии с предыдущим примером, обучите нейронную сеть, у которой следущие слои:\n",
    "\n",
    "- Сверточный слой с 10 ядрами размером 5\n",
    "- Функция активации ReLU\n",
    "- Уменьшить картинку в 2 раза (по каждому измерению)\n",
    "- Сверточный слой с 20 ядрами размером 5\n",
    "- Функция активации ReLU\n",
    "- Уменьшить картинку в 2 раза (по каждому измерению)\n",
    "- Полносвязный слой со 128 нейронами\n",
    "- Функция активации ReLU\n",
    "- Выходной слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff943752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850ed4e",
   "metadata": {},
   "source": [
    "### Наконец напишем нормальный класс для обучения \n",
    "\n",
    "прям как мы любим)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "792a271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec43aa",
   "metadata": {},
   "source": [
    "Логировать будем в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7fc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 criterion, \n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 batch_size: int = 16,\n",
    "                 log_wandb: bool = True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.log_wandb = log_wandb\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, path, epoch, loss):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),  \n",
    "            'loss': loss\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, pin_memory=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, pin_memory=True, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_bar = tqdm(train_loader)\n",
    "            for image, labels in train_bar:\n",
    "                image, labels = image.to(self.device), labels.to(self.device)\n",
    "                prediction = model(image)\n",
    "                loss = criterion(prediction, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_bar.set_description(f'Loss {loss.data}')\n",
    "                \n",
    "                if self.log_wandb:\n",
    "                    wandb.log({\"train loss\": loss})\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            val_accuracy = []\n",
    "            for image, labels in tqdm(val_loader):\n",
    "                image, labels = image.to(self.device), labels.to(self.device)\n",
    "                prediction = model(image)\n",
    "                loss = criterion(prediction, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accuracy.extend((torch.argmax(prediction, dim=-1) == labels).cpu().numpy())\n",
    "                \n",
    "\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_acc = np.mean(val_accuracy)\n",
    "            \n",
    "            if self.log_wandb:\n",
    "                    wandb.log({\"val_loss\": val_loss,\n",
    "                              \"val_acc\": val_acc})\n",
    "            print(f'Validation loss {val_loss} after epoch {epoch}')\n",
    "            print(f'Validation accuracy {val_acc} after epoch {epoch}')\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(\"./best_checkpoint.pt\", epoch, val_loss)\n",
    "                best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7606c",
   "metadata": {},
   "source": [
    "Затестим на одном VGG блоке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3d3c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2704, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.prediction(x)\n",
    "        return x\n",
    "    \n",
    "model_baseline = ModelBaseline()\n",
    "optimizer = torch.optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a08ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model_baseline, optimizer, criterion, trainset, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf69d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masyakarpova\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/asyakarpova/sem12/runs/ao4c4osj\" target=\"_blank\">light-snow-3</a></strong> to <a href=\"https://wandb.ai/asyakarpova/sem12\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "config = dict(model='VGG',\n",
    "             learning_rate=0.001,\n",
    "             data='CIFAR10')\n",
    "\n",
    "\n",
    "wandb.init(project=\"sem12\",\n",
    "           notes=\"cnn-vgg\",\n",
    "           config=config)\n",
    "\n",
    "\n",
    "wandb.watch(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d32a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a529e4d8014f18817ea45ba5458cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc4531e2d4b484db020a2bd631c12fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.5034754077911376 after epoch 0\n",
      "Validation accuracy 0.4548 after epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddb874bf33647aa85ba9d584f122adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bc02a3b0124b83959218591fac23aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.3202824522972108 after epoch 1\n",
      "Validation accuracy 0.5208 after epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddce02726af4081b27d290a277b9d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b0120f40d48d0b818dc6b0c098612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.2155629776000976 after epoch 2\n",
      "Validation accuracy 0.5686 after epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41832f46daa34d0fb42681531b32cb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d174defca9bc4c23a98bfb08bce9430d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.1809065402030945 after epoch 3\n",
      "Validation accuracy 0.5775 after epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741c9c047da544a69db78065fd063ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75d2e3eb3fa44bfac144c7f8fe08ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.096249002122879 after epoch 4\n",
      "Validation accuracy 0.6113 after epoch 4\n"
     ]
    }
   ],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce086c4",
   "metadata": {},
   "source": [
    "Восстановим чекпоинт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0419aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best_checkpoint.pt')\n",
    "model = \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a589f1",
   "metadata": {},
   "source": [
    "### Аугментация изображений \n",
    "\n",
    "С помощью поворотов, отражения, добавления шума, сдвигов и других преобразований, картинка немного меняется, однако сохраняет свою прежнюю метку. С помощью функции Compose можно объединять несколько трансформаций изображения, а потом применять их при чтении датасета. Полный список аугментаций доступен [тут](https://pytorch.org/vision/stable/transforms.html). Изучите его и поэкспериментируйте с различными трансформациями изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a840b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=True,                             \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    valset, \n",
    "    batch_size=16,\n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c2268",
   "metadata": {},
   "source": [
    "### Как еще улучшить качество модели?\n",
    "\n",
    "Особенность VGG блока заключается в том, что повышение качества работы сети достигается увеличением числа последовательных блоков. При этом число фильтров в каждом новом блоке в два раза больше, чем в предыдущем. Давайте попробуем объединить 3 VGG блока.\n",
    "\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "Если сеть имеет сложную архитектуру, то возможно переобучение - процесс, в котором модель слишком сильно подстраивается под тренировочную выборку и потом дает заниженное качество на тестовой. Для борьбы с этим в нейросетях используют Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e530c4",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://files.ai-pool.com/a/59df0e2cc98add51893f784916195478.png\" alt=\"bn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4666",
   "metadata": {},
   "source": [
    "#### BatchNorm\n",
    "\n",
    "https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Для ускорения и стабилизации обучения добавляют BatchNorm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed177cc",
   "metadata": {},
   "source": [
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" alt=\"bn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91c03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Conv2d(16, 32, 3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, 3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Conv2d(32, 64, 3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 4 * 4, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73a3d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "123b7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, trainset, valset, log_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08456df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9zq0db8b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5783... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">clean-elevator-1</strong>: <a href=\"https://wandb.ai/asyakarpova/sem12_vgg/runs/9zq0db8b\" target=\"_blank\">https://wandb.ai/asyakarpova/sem12_vgg/runs/9zq0db8b</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211207_162303-9zq0db8b/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9zq0db8b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/asyakarpova/sem12_vgg/runs/l5cz0n9r\" target=\"_blank\">cool-rain-2</a></strong> to <a href=\"https://wandb.ai/asyakarpova/sem12_vgg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = dict(model='improved VGG',\n",
    "             learning_rate=0.001,\n",
    "             data='CIFAR10')\n",
    "\n",
    "\n",
    "wandb.init(project=\"sem12_vgg\",\n",
    "           notes=\"cnn-vgg\",\n",
    "           config=config)\n",
    "\n",
    "\n",
    "wandb.watch(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5af595",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d6765",
   "metadata": {},
   "source": [
    "### Задание 3 (Домашка). Реализуйте архитектуру с картинки. \n",
    "\n",
    "Детали (типо размера сверток) возьмите из оригинальной статьи. https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-2.png\" alt=\"bn\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df087d02",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Задачу ниже точно лучше решать с GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5246eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip && unzip kagglecatsanddogs_3367a.zip > /dev/null\n",
    "    \n",
    "#!rm -rf ./PetImages/Cat/666.jpg ./PetImages/Dog/11702.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d70c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor \n",
    "\n",
    "dataset = ImageFolder(\n",
    "    \"./PetImages\", \n",
    "    transform=Compose(\n",
    "        [\n",
    "            Resize((224, 224)), \n",
    "            ToTensor(), \n",
    "            Normalize((0.5, 0.5, 0.5), (1, 1, 1)), \n",
    "        ]\n",
    "    )\n",
    ")\n",
    "trainset, valset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image = np.random.choice(glob.glob(\"./PetImages/*/*.jpg\"))\n",
    "plt.imshow(plt.imread(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "# Загрузить предобученную сеть -- pretrained=True\n",
    "model = resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5aaea0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10be4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5c1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, trainset, valset, log_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0081c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65eff5",
   "metadata": {},
   "source": [
    "**Main sources**:\n",
    "\n",
    " - https://github.com/m12sl/dl-hse-2021/blob/main/03-training/seminar.ipynb\n",
    "    \n",
    " - https://github.com/hse-ds/iad-deep-learning/blob/master/2021/seminars/sem03/sem03_task.ipynb\n",
    " \n",
    " - https://github.com/hse-ds/iad-deep-learning/blob/master/2021/seminars/sem05/sem05_task.ipynb\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
